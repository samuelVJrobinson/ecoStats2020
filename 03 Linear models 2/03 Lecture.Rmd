---
title: "Linear models 2"
subtitle: "More bells and whistles"
author: "Samuel Robinson, Ph.D."
date: "October 15, 2020"
output: 
  beamer_presentation:
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
df_print: kable
header-includes: 
  \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  \useinnertheme{circles}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_classic())
library(ggpubr)

set.seed(123)

#Plots of mtcars data

#Continuous
p1 <- ggplot(arrange(mtcars,disp),aes(x=disp,y=mpg))+geom_point()+geom_smooth(formula=y~x,method='lm',se=FALSE,col='orange')

#Categorical - 3 levels
p2 <- ggplot(arrange(mtcars,gear,mpg),aes(x=factor(gear),y=mpg))+ 
  geom_point(position=position_jitter(width=0.05))+labs(x='gears')+
  geom_point(aes(y=mpg),stat='summary',fun=mean,col='orange',size=3) #Mean only

#Interaction
p3 <- ggplot(mtcars,aes(x=disp,y=mpg,col=factor(gear)))+geom_point()+labs(col='gears')+
  geom_smooth(formula=y~x,method='lm',se=FALSE)+
  scale_colour_manual(values=c('blue','purple','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

#Fit models for later use
modp1 <- lm(mpg~disp,data=arrange(mtcars,disp))
modp2 <- lm(mpg~factor(gear),data=arrange(mtcars,am,disp))
modp3 <- lm(mpg~disp*factor(gear),data=mtcars)

```


## Motivation {.build}

> - _I have 2+ groups of data, and I want to know whether the means are different_

> - _I have 2+ groups of bivariate data, and I want to know whether the relationships differ between groups_

> - __How do we know if any of this matters?__

```{r examplePlots, echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
ggarrange(p2,p3,ncol=2) #Display mtcars data
```

## Categorial data, 3 categories

::: columns

:::: column

```{r, fig.height=3, fig.width=3}

b0Lab <- paste('b[0] == ',round(coef(modp2)[1],2))
b1Lab <- paste('b[1] == ',round(coef(modp2)[2],2))
b2Lab <- paste('b[2] == ',round(coef(modp2)[3],2))


  p2 +
    annotate('text', x=1.5, y = coef(modp2)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
    geom_hline(yintercept=coef(modp2)[1],linetype='dashed',col='blue') +
    
    annotate('text', x=2.35, y = (coef(modp2)[1] + coef(modp2)[2])*1.05, col='blue', label=b1Lab, parse=TRUE)+
    annotate('segment', x = 2.25, xend = 2.25, y= coef(modp2)[1], yend = coef(modp2)[1] + coef(modp2)[2],linetype='dashed',col='blue') +
    
    annotate('text', x=3.35, y = (coef(modp2)[1] + coef(modp2)[3])*1.05, col='blue', label=b2Lab, parse=TRUE)+
    annotate('segment', x = 3.25, xend = 3.25, y= coef(modp2)[1], yend = coef(modp2)[1] + coef(modp2)[3],linetype='dashed',col='blue') +
    
    theme(axis.title.x.bottom=element_text(colour='darkturquoise'),
            axis.text.x.bottom=element_text(colour='darkturquoise'))
```

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_5} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

::::

:::: column

The more factor levels, the more coefficients:

>- $mpg$ is the thing you're interested in predicting
>- $\textcolor{orange}{\hat{mpg}}$ is the _predicted value_ of $mpg$
>- $\textcolor{darkturquoise}{gear}$ is the _predictor_ of _mpg_
  >- set of 0s and 1s
  >- $\textcolor{darkturquoise}{gears_4}=$ "is this data point from a 4-gear car?"
>- $\textcolor{blue}{b_0}=$ _intercept_
>- $[\textcolor{blue}{b_1},\textcolor{blue}{b_2}]=$ are _coefficients_ for $\textcolor{darkturquoise}{gears}$

::::

:::

## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#Formula structure: y ~ x
mod1 <- lm(mpg ~ factor(gear), #mpg depends on gears
           data = mtcars) #Name of the dataframe containing mpg & gears
summary(mod1)
```

## Dummy variables
\tiny
```{r, echo=TRUE} 
mod1Matrix <- model.matrix(mod1) #Get model matrix (columns used to predict mpg)
head(mod1Matrix,28) #Show first 28 rows of model matrix
```

## Interactions

What if the slopes _and_ intercepts differ between groups?

```{r, fig.height=3, fig.width=5}
  p3 
```

## Interactions

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
b0Lab <- paste('b[0] == ',round(coef(modp3)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp3)[2],3))
b4Lab <- paste('b[4] == ',round(coef(modp3)[4],3))
b5Lab <- paste('b[5] == ',round(coef(modp3)[6],3)) 

p3 + xlim(0,NA) +
  #Text/lines for gear==3
  annotate('text', x=250, y = coef(modp3)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
  annotate('segment', x = 400, xend = 400, y= coef(modp3)[c(1,2)] %*% c(1,400), yend = (coef(modp3)[c(1,2)] %*% c(1,400))*1.2,linetype='dashed',col='blue') +
  annotate('segment', x = 300, xend = 400, y= (coef(modp3)[c(1,2)] %*% c(1,400))*1.2, yend = (coef(modp3)[c(1,2)] %*% c(1,400))*1.2,linetype='dashed',col='blue') + 
  annotate('text', x=400, y = (coef(modp3)[c(1,2)] %*% c(1,400))*1.25, col='blue', label=b1Lab, parse=TRUE) +
  geom_hline(yintercept=coef(modp3)[1],linetype='dashed',col='blue') +
  geom_abline(intercept=coef(modp3)[1],slope=coef(modp3)[2],col='blue',linetype='dashed') +
  #Text/lines for gear==5
  annotate('text', x=250, y = sum(coef(modp3)[c(1,4)])*1.05, col='red', label=b4Lab, parse=TRUE) +
  annotate('segment', x = 250, xend = 250, y= coef(modp3)[c(1,2,4,6)] %*% c(1,250,1,250), 
           yend = (coef(modp3)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, linetype='dashed',col='red') +
  annotate('segment', x = 180, xend = 250, y= (coef(modp3)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2,
           yend = (coef(modp3)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, linetype='dashed',col='red') +
  annotate('text', x = 350, y = (coef(modp3)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, col='red', label=b5Lab, parse=TRUE) +
  geom_hline(yintercept=sum(coef(modp3)[c(1,4)]),linetype='dashed',col='red') +
  geom_abline(intercept=sum(coef(modp3)[c(1,4)]),slope=sum(coef(modp3)[c(2,6)]),col='red',linetype='dashed')
```

::::

:::: column

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp}\\
& + \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_3}\textcolor{darkturquoise}{gears_5}\\
& + \textcolor{blue}{b_4}\textcolor{darkturquoise}{(disp\times gears_4)}\\
& + \textcolor{blue}{b_5}\textcolor{darkturquoise}{(disp\times gears_5)}\\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

>- Interactions occur when predictors are _multiplied_
>- In this case, $\textcolor{darkturquoise}{disp}$ is multiplied by $\textcolor{darkturquoise}{gears_4}$ and $\textcolor{darkturquoise}{gears_5}$
::::

:::

## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#Formula structure: y ~ x
mod2 <- lm(mpg ~ disp*factor(gear), #mpg depends on disp interacted with gears
           data = mtcars) #Name of the dataframe 
summary(mod2)
```

## Dummy variables
\tiny
```{r, echo=TRUE} 
mod2Matrix <- model.matrix(mod2) #Get model matrix (columns used to predict mpg)
colnames(mod2Matrix) <- gsub('factor\\(gear\\)','gear',colnames(mod2Matrix)) #Shorten colnames
head(mod2Matrix,28) #Show first 28 rows of model matrix
```

## How do I know if any of this matters?

>- drop-1 (Type III) ANOVA for _entire factors_
  >- e.g. "Does adding gear matter?"
>- Wald t-scores/Z-scores for _levels of factors_
  >- e.g. "Is gear3 different from gear4?"
>- __p-values are only meaningful if the model assumptions are valid__

## drop-1 ANOVA

\tiny
```{r, echo=TRUE} 
#mpg depends on gears
mod1 <- lm(mpg ~ factor(gear), data = mtcars)
drop1(mod1,test='F') #Effect of gears is very strong
```

```{r, echo=TRUE} 
#mpg depends on disp
mod2 <- lm(mpg ~ disp, data = mtcars)
drop1(mod2,test='F') #Effect of disp is also very strong
```

## drop-1 ANOVA
\tiny
```{r, echo=TRUE} 
#mpg depends on disp and gear
mod3 <- lm(mpg ~ disp + factor(gear), data = mtcars)
drop1(mod3,test='F') #Effect of disp is very strong, and erases the effect of gear
```

```{r, echo=TRUE} 
#mpg depends on disp interacted with gear
mod4 <- lm(mpg ~ disp*factor(gear), data = mtcars)
drop1(mod4,test='F') #Interaction effect is strong. Why are disp and gear not shown?
```

## Wald t-scores
>- Wald t-scores are shown in model `summary`
>- t-score (aka Z-score) = mean$\div$SD 
>- p-value comes from Student's t-distribution (similar to Normal, but has longer tails depending on sample size)

\tiny

```{r, echo=TRUE} 
summary(mod1)
```

## Comparing between intercepts

>- If you're comparing between many intercepts, you need to account for _multiple comparisons_
>- One common method: Tukey's Honestly Significant Difference (HSD)

\tiny

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(multcomp) #Loads the multcomp package (needs to be installed first)
mod1Comp <- glht(mod1, linfct = mcp('factor(gear)'='Tukey')) #Fits multcomp object using gear
summary(mod1Comp)
```