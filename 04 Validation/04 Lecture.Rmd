---
title: "Linear models 3"
subtitle: "Models behaving badly"
author: "Samuel Robinson, Ph.D."
date: "October 15, 2020"
output: 
  beamer_presentation:
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
df_print: kable
header-includes: 
  \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  \useinnertheme{circles}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_classic())
library(ggpubr)

set.seed(123)

#Functions
logit <- function(x) log(x/(1-x))
invLogit <- function(x) exp(x)/(1+exp(x))

#Plots of mtcars data
#Continuous
p1 <- ggplot(arrange(mtcars,disp),aes(x=disp,y=mpg))+geom_point()+geom_smooth(formula=y~x,method='lm',se=FALSE,col='orange')

#Interaction
p3 <- ggplot(mtcars,aes(x=disp,y=mpg,col=factor(gear)))+geom_point()+labs(col='gears')+
  geom_smooth(formula=y~x,method='lm',se=FALSE)+
  scale_colour_manual(values=c('blue','purple','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

#Fit models for later use
modp1 <- lm(mpg~disp,data=arrange(mtcars,disp))
modp3 <- lm(mpg~disp*factor(gear),data=mtcars)

#Generate data that violate lm assumptions:
n <- 100
x <- runif(n,-10,10)
yhat <- 3 - 1*x #Expected value
y0 <- yhat + rnorm(n,0,2) #OK
y1 <- yhat + 0.5*x^2 + rnorm(n,0,2) #Polynomial function
y2 <- rpois(n,exp(yhat))  #Poisson process
y3 <- rbinom(n,1,invLogit(yhat))  #Binomial process
y4 <- yhat + rexp(n,0.5) #Exponential residuals
y5 <- yhat + rnorm(n,0,exp(0.5+0.15*x)) #Non-constant variance
d1 <- data.frame(x,yhat,y0,y1,y2,y3,y4,y5)

```


## Motivation 

- Are my model results reliable?
  - Residual checks
  - Transformations
  - Scaling
  - Collinearity
- How do I tell if terms are important or not?
  - Drop-1 ANOVA
  - Wald t-tests
- How much stuff should I put into my model?
  - Causal modeling vs Machine learning
  - Avoiding a fishing expedition (model weights, stepwise selection)
  
## Are my model results reliable?

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
meanDisp <- mean(mtcars$disp) #Mean displacement
y_meanDisp <- coef(modp1) %*% c(1,meanDisp) #Predicted y at mean displacement
b0Lab <- paste('b[0] == ',round(coef(modp1)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp1)[2],3)) 

p1 + theme(axis.title.x.bottom=element_text(colour='darkturquoise'),axis.text.x.bottom=element_text(colour='darkturquoise'))
```

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

::::

:::: column

There are 3 main assumptions to this model:

1. The relationship between $\textcolor{darkturquoise}{disp}$ and $mpg$ is linear
2. $mpg$ (the data) is Normally distributed around $\textcolor{orange}{\hat{mpg}}$ (the line)
3. $\textcolor{red}{\sigma}$ is the same everywhere

This is pretty easy to see if you only have 1 variable, but...

::::

::: 

## What if I have many variables?

```{r, fig.height=3, fig.width=5}
p3
```

Difficult to see if the assumptions are met

## Solution: residual checks

Some common ways of checking the assumptions: __residual plots__

\tiny

```{r, echo=TRUE, out.width='100%', fig.asp=0.5} 
mod1 <- lm(mpg~disp*factor(gear),data=mtcars)
par(mfrow=c(1,2),mar=c(3,3,1,1)+1)
plot(mod1, which=c(1,2))
```

\normalsize

1. Points in Plot 1 should show _no pattern_ (shotgun blast)
2. Points in Plot 2 should be _roughly_ on top of the 1:1 line

## Problem 1: Non-linear relationship

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(x,y1))+geom_point()+geom_smooth(method='lm',se=FALSE,formula=y~x,col='orange')
```

$y1$ clearly follows a hump-shaped relationship

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m1 <-lm(y1~x,data=d1) 
plot(m1, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::