---
title: "GLMs: Validation"
subtitle: "Models behaving badly: Part 2!"
author: "Samuel Robinson, Ph.D."
date: "December 3, 2020"
output: 
  beamer_presentation:
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
df_print: kable
header-includes: 
  - \usepackage{tikzit}
  - \input{styles.tikzstyles}
  - \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  - \useinnertheme{circles}
---

```{r setup, include=FALSE}
#Trick to get smaller R code size with out resorting to LaTeX text sizes
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message=TRUE, warning=TRUE, size = 'footnotesize')
library(MASS) #glm.nb
library(tidyverse)
theme_set(theme_classic())
library(ggpubr)
library(knitr)
# library(kableExtra)
# library(latex2exp)

set.seed(123)

#Functions
logit <- function(x) log(x/(1-x))
invLogit <- function(x) exp(x)/(1+exp(x))

#Generate data that violate lm assumptions:
n <- 100
x <- runif(n,-10,10)
yhat <- 1 - 0.2*x #Expected value
y0 <- yhat + rnorm(n,0,2) #OK
y1 <- rpois(n,exp(yhat))  #Poisson process
y2 <- rnbinom(n,mu=exp(yhat),size=1)  #NB process
# y2 <- rbinom(n,1,invLogit(yhat))  #Binomial process

d1 <- data.frame(x,yhat,y0,y1,y2) #Dataframe
rm(n,x,y0,y1,y2,yhat)

```

## Motivation

- Are my model results reliable?
  - Residual checks
  - Overdispersion
  - Zero-inflation
- Model selection - which terms should I use?
  - ML vs REML
  - log-likelihood, $\chi^2$ tests, and AIC
- Other things
  - Binomial GLMs with >1 trial
  - Offsets in count models
  - $R^2$ for GLMs
- Show-and-tell!

## Problem 1: Residual checks

::: columns

:::: column

- In LMs, residual checks are used to make sure that:
1. Terms are linearly related
2. Generating process is valid
3. Variance is constant
- "Regular" residuals don't work this way for GLMs!

```{r}
m1 <- glm(y1~x,data=d1,family='poisson')
plot(y1~x,data=d1,pch=19)
lines(sort(d1$x),exp(cbind(rep(1,nrow(d1)),sort(d1$x)) %*% coef(m1)),lwd=1,col='blue')
# x, yhat, y
resNo <- 81 #Line number to use as example
lines(rep(d1$x[resNo],2),c(exp(d1$yhat[resNo]),d1$y1[resNo]),col='red')
r1 <- d1$y1[resNo] - exp(d1$yhat[resNo])
text(x=d1$x[resNo],mean(c(exp(d1$yhat[resNo]),y=d1$y1[resNo])),
     labels=paste('Residual = +',round(r1,3)),adj=-0.1,col='red',cex=3)
```

::::

:::: column

```{r}
plot(predict(m1),residuals(m1,type='response'),xlab='Predicted value',ylab='Residuals');abline(h=0,lty='dashed')
points(x=predict(m1)[resNo],y=residuals(m1,type='response')[resNo],col='black',pch=19)
lines(x=rep(predict(m1)[resNo],2),y=c(0,residuals(m1,type='response')[resNo]),col='red')
```

```{r}
colVec <- rep('black',nrow(d1)); colVec[resNo] <- 'red'
pchVec <- rep(1,nrow(d1)); pchVec[resNo] <- 19
qqnorm(residuals(m1,type='response'),col=colVec,pch=pchVec); qqline(residuals(m1,type='response')) 

```


:::: 

:::

## There are _many_ kinds of residuals!

In addition to _response_ (regular) residuals there are:

- Working residuals
- Pearson residuals
- __Deviance residuals__

Deviance residuals use _likelihood_:

\begin{equation*}
r_{dev} = sign(y-\hat{y})\sqrt{2(log(L(y|\theta_s))-log(L(y|\theta))))}
\end{equation*}

- This may look scary, but R does this all for you!
- These are analogous to regular residuals in LMs
- For more about the different kinds of residuals, see [here](https://www.datascienceblog.net/post/machine-learning/interpreting_generalized_linear_models/)

## Solution: use deviance residuals for GLMs

::: columns

:::: column

Keep in mind: 

- Residuals from GLMs will never be as "pretty" as those from LMs
- _Especially_ true for:
  - Binomial GLMs
  - Poisson/Negative Binomial GLMs with many zeros

```{r}
m1 <- glm(y1~x,data=d1,family='poisson')
plot(y1~x,data=d1,pch=19)
lines(sort(d1$x),exp(cbind(rep(1,nrow(d1)),sort(d1$x)) %*% coef(m1)),lwd=1,col='blue')
# x, yhat, y
resNo <- 81 #Line number to use as example
lines(rep(d1$x[resNo],2),c(exp(d1$yhat[resNo]),d1$y1[resNo]),col='red')
r1 <- d1$y1[resNo] - exp(d1$yhat[resNo])
text(x=d1$x[resNo],mean(c(exp(d1$yhat[resNo]),y=d1$y1[resNo])),
     labels=paste('Residual = +',round(r1,3)),adj=-0.1,col='red',cex=3)
```

::::

:::: column

```{r}
plot(predict(m1),residuals(m1,type='deviance'),xlab='Predicted value',ylab='Deviance Residuals');abline(h=0,lty='dashed')
points(x=predict(m1)[resNo],y=residuals(m1,type='deviance')[resNo],col='black',pch=19)
lines(x=rep(predict(m1)[resNo],2),y=c(0,residuals(m1,type='deviance')[resNo]),col='red')
```

```{r}
colVec <- rep('black',nrow(d1)); colVec[resNo] <- 'red'
pchVec <- rep(1,nrow(d1)); pchVec[resNo] <- 19
qqnorm(residuals(m1,type='deviance'),col=colVec,pch=pchVec); qqline(residuals(m1,type='deviance')) 

```


:::: 

:::

## Problem 2: Overdispersion

::: columns

:::: column

- Binomial and Poisson families have __no__ variance term (e.g. _SD_).
- Sometimes this assumption doesn't work!
- This is very common for Poisson models

::::

:::: column

```{r}
m2 <- glm(y2~x,data=d1,family='poisson')

do.call('cbind',predict(m2,se.fit=TRUE)[c(1:2)]) %>% data.frame() %>% 
  mutate(x=d1$x,y=d1$y2,upr=fit+se.fit*2,lwr=fit-se.fit*2) %>% 
  mutate(across(c(fit,upr,lwr),exp)) %>% arrange(x) %>% 
  ggplot(aes(x=x))+geom_point(aes(y=y))+
  geom_ribbon(aes(ymax=upr,ymin=lwr),alpha=0.3)+
  geom_line(aes(y=fit))
```

Example: data are much more variable than the predictions from the model

:::: 

:::

## Problem: Overdispersion

```{r, size='tiny'}
summary(m1)
```

- In a regular Poisson or Binomial model, Residual deviance $\div$ Degrees of Freedom should be $\sim$ 1
- Residual deviance is the sum of all deviance from the model
- This model looks OK (`r round(m1$deviance,2)` $\div$ `r round(m1$df.residual,2)` = `r with(m1, round(deviance/df.residual,2))`)

## Problem: Overdispersion

```{r, size='tiny'}
m2 <- glm(y2~x,data=d1,family='poisson')
summary(m2)
```

- This model does __not__ look OK (`r round(m2$deviance,2)` $\div$ `r round(m2$df.residual,2)` = `r with(m2, round(deviance/df.residual,2))`)
- Generated using Negative Binomial, but fit to Poisson

## Causes

Overdispersion can be caused by different things:

- Using the wrong probability distribution
  - e.g. Poisson, but should be Negative Binomial
- Lots of zeros in count data
  - e.g. Very short observation period
- Leaving out an important term
  - e.g. An important _interaction_ term was omitted
- Random effects\footnotemark not accounted for
  - e.g. Data collected at different sites, but ignored
  
\footnotetext{Random effects discussed later}

## Solutions for overdispersion

Try the following (in this order):

1. Consider terms that may have been left out
    1. Fixed effects
    2. Random effects
2. Try distributions that account for overdispersion
    1. Negative Binomial, Beta Binomial, Zero-inflated Poisson\footnotemark 
    2. Quasi-binomial\footnotemark[\value{footnote}] and quasi-poisson\footnotemark[\value{footnote}]
    3. Transform counts to presence/absence
3. Lower your expectations, and use a lower critical p-value (e.g. 0.01 instead of 0.05)
4. Design a better study :(

\footnotetext{These can be annoying to deal with, so avoid if possible}

## Zero-inflation: drunk monks

An analogy: 

1. Monks at a monastary make copies of manuscripts. Most days they make very few (0 or 1), but occasionally they make many (2-5)
2. Some days they decide to try out the beer that's been brewing in the cellar! No manuscripts get made on those days.
3. The number of manuscripts made (per day) follows a _zero-inflated Poisson distribution_

This is _mixture_ of a Poisson and a Binomial:

<!-- \begin{equation*} -->
<!-- ZIPoisson(y | \lambda, \phi) = -->
<!-- \begin{cases} -->
<!-- Poisson(0|\lambda)\text{ OR }Binomial(0|\phi) & \text{if } y = 0 \\ -->
<!-- Poisson(y|\lambda) & \text{if } y > 0 -->
<!-- \end{cases} -->
<!-- \end{equation*} -->

\begin{tikzpicture}
	\begin{pgfonlayer}{nodelayer}
		\node [style=rectangle] (0) at (-8.25, 0) {Binomial ($\phi$)};
		\node [style=rectangle] (5) at (-3.75, 1) {No work};
		\node [style=rectangle] (6) at (-3.75, -1) {Work};
		\node [style=rectangle] (8) at (0.5, -1) {Poisson ($\lambda$)};
		\node [style=invisRect] (9) at (-8.5, 1.5) {Get drunk?};
		\node [style=invisRect] (10) at (1.75, -3.25) {};
		\node [style=invisRect] (11) at (7, 1) {0 manuscripts};
		\node [style=invisRect] (12) at (7, -1) {1+ Manuscripts};
	\end{pgfonlayer}
	\begin{pgfonlayer}{edgelayer}
		\draw [style=oneWay] (0) to (6);
		\draw [style=oneWay] (6) to (8);
		\draw [style=oneWay] (0) to (5);
		\draw [style=oneWay] (5) to (11);
		\draw [style=oneWay] (8) to (12);
		\draw [style=oneWay] (8) to (11);
	\end{pgfonlayer}
\end{tikzpicture}


## Zero-inflation: graphical model

```{r, fig.width=5, fig.height=5}
n <- 200
data.frame(p=rpois(n,2),zi=rbinom(n,1,0.1)) %>% 
  mutate(proc=ifelse(zi==1,'Extra Zeros','Poisson'),p=ifelse(zi==1,0,p)) %>% 
  group_by(proc,p) %>% count() %>% ungroup() %>% 
  mutate(proc=factor(proc,levels=c('Extra Zeros','Poisson'))) %>% 
  ggplot(aes(x=factor(p)))+geom_col(aes(y=n,fill=proc))+
  labs(x='Counts',y='Frequency',fill='Process')+scale_fill_manual(values=c('red','black'))+
  theme(legend.position = 'bottom')

```


## R actually uses _log_ Likelihood

::: columns

:::: column

- Probabilities multiplied together quickly become _very small_
- Computers can't distinguish between extremely big or small numbers
- Therefore, it uses _log-likelihoods_ (also easier to calculate)

::::

:::: column

```{r, echo=FALSE, eval = TRUE, fig.width= 3, fig.height = 2}
#Uses logit-phi (-Inf to +Inf) rather than phi (0 to 1)
llfun2 <- function(lphi){
  phi <- invLogit(lphi)
  ll <- dbinom(1,1,phi)*dbinom(1,1,phi)*dbinom(0,1,phi)
  return(ll)
} 

ggplot() + geom_function(fun=llfun2) + xlim(-5,8) + labs(x=expression(paste('logit(',phi,')')),y=expression(paste('Likelihood(H,H,T|',phi,')'))) + geom_vline(xintercept=logit(2/3),linetype='dashed')

```

```{r, echo=FALSE, eval = TRUE, fig.width= 3, fig.height =2}

ggplot() + geom_function(fun=~log(llfun2(.))) + xlim(-5,8) + labs(x=expression(paste('logit(',phi,')')),y=expression(paste('log-likelihood(H,H,T|',phi,')'))) + geom_vline(xintercept=logit(2/3),linetype='dashed')

```

::::

:::