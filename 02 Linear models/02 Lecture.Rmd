---
title: "Linear models"
subtitle: "How do they work?"
author: "Samuel Robinson, Ph.D."
date: "October 1, 2020"
output: beamer_presentation
df_print: kable
header-includes: 
  \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_classic())
library(ggpubr)

#Functions

#Plots of mtcars data

#Continuous
p1 <- ggplot(mtcars,aes(x=disp,y=mpg))+geom_point()+geom_smooth(formula=y~x,method='lm',se=FALSE,col='orange')

#Categorical
p2 <- ggplot(mtcars,aes(x=factor(am,labels=c('auto','manual')),y=mpg))+ 
  geom_point(position=position_jitter(width=0.05))+labs(x='trans')+
  geom_pointrange(stat='summary',fun.data=mean_se,fun.args = list(mult = 2),col='orange')

#Interaction - use this next lecture
p3 <- ggplot(mtcars,aes(x=disp,y=mpg,col=factor(am,labels=c('auto','manual'))))+geom_point()+labs(col='trans')+
  geom_smooth(formula=y~x,method='lm',se=FALSE)+
  scale_colour_manual(values=c('blue','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

#Fit models for later use
modp1 <- lm(mpg~disp,data=mtcars)
modp2 <- lm(mpg~am,data=mtcars)
modp3 <- lm(mpg~disp*am,data=mtcars)

```


## Motivation

- _I have some bivariate data (2 things measured per row), and I want to know if they're related to each other_

- _I have 2+ groups of data, and I want to know whether the means are different_

<!-- Use this next lecture -->
<!-- - _I have 2+ groups of bivariate data, and I want to know whether the relationships differ between groups_ -->

```{r examplePlots, echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
ggarrange(p1,p2,ncol=2) #Display mtcars data
```

## Model terminology {.build}

- All linear models take the form:
\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{x_1} + \textcolor{blue}{b_2}\textcolor{darkturquoise}{x_2} ... + \textcolor{blue}{b_i}\textcolor{darkturquoise}{x_i} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

- $y$ is the thing you're interested in predicting
- $\textcolor{orange}{\hat{y}}$ is the _predicted value_ of $y$
- $\textcolor{darkturquoise}{x_1...x_i}$ are _predictors_ of _y_
- $\textcolor{blue}{b_1...b_i}$ are _coefficients_ for each predictor $\textcolor{darkturquoise}{x_i}$
- $\textcolor{blue}{b_0}$ is the _intercept_, a coefficient that doesn't depend on predictors
- $y\sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})$ means:
  - "$y$ follows a Normal distribution with mean $\textcolor{orange}{\hat{y}}$ and SD $\textcolor{red}{\sigma}$"

This may look terrifying, but let's use a simple example:

## Example

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
meanDisp <- mean(mtcars$disp) #Mean displacement
y_meanDisp <- coef(modp1) %*% c(1,meanDisp) #Predicted y at mean displacement
b0Lab <- paste('b[0] == ',round(coef(modp1)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp1)[2],3)) 

p1 + xlim(0,NA) + 
  geom_hline(yintercept=coef(modp1)[1],linetype='dashed',col='blue') +
  geom_abline(intercept=coef(modp1)[1],slope=coef(modp1)[2],col='orange',linetype='dashed') +
  annotate('text', x=meanDisp, y = coef(modp1)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
  annotate('segment', x = meanDisp, xend = meanDisp, y= y_meanDisp, yend = y_meanDisp*1.2,linetype='dashed',col='blue') +
  annotate('text', x=meanDisp*1.05, y = y_meanDisp*1.25, col='blue', label=b1Lab, parse=TRUE) +
  annotate('segment', x = (y_meanDisp*1.2-coef(modp1)[1])/coef(modp1)[2] , xend = meanDisp, y= y_meanDisp*1.2 ,yend = y_meanDisp*1.2,
           linetype='dashed',col='blue') +

    theme(axis.title.x.bottom=element_text(colour='darkturquoise'),axis.text.x.bottom=element_text(colour='darkturquoise'))
```

<!-- $$ \hat{mpg} = b_0 + b_1{disp} $$ -->
<!-- $$ mpg \sim Normal(\hat{mpg},\sigma) $$  -->

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

::::

:::: column

- $mpg$ is the thing you're interested in predicting
- $\textcolor{orange}{\hat{mpg}}$ is the _predicted value_ of $mpg$
- $\textcolor{darkturquoise}{disp}$ is the _predictor_ of _mpg_
- $\textcolor{blue}{b_0}$ is the _intercept_, $\textcolor{blue}{b_1}$ is the _coefficient_ for $\textcolor{darkturquoise}{disp}$
- $mpg\sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})$ means:
  - "$mpg$ follows a Normal distribution with mean $\textcolor{orange}{\hat{mpg}}$ and SD $\textcolor{red}{\sigma}$"
- $\textcolor{red}{\sigma}$ isn't displayed on the figure. Where is it?

::::

:::

## How do I get R to fit this model?

`lm` is one of the main functions used for linear modeling:

## A challenger approaches!

- Simulate your own data with _3_ levels, rather than 2
- Use `lm` to fit a model to the data you just simulated



## Why do we call them "linear models"?

__To answer this, we need a brief review of matrix algebra__

- This is a matrix: $$ A = \begin{bmatrix}
  1 & 4 & 7 \\
  2 & 5 & 8 \\
  3 & 6 & 9
 \end{bmatrix}
$$

- This is a vector $$ b = \begin{bmatrix}
  1  \\
  2 \\
  3 
 \end{bmatrix}
$$

- Multiplying them requires a _transposition_ (flipping along main diagonal, denoted by $^\top$)

$$ A \times b = Ab^\top = 1 \times \begin{bmatrix}
  1  \\
  2 \\
  3 
 \end{bmatrix} + 2 \times \begin{bmatrix}
  4  \\
  5  \\
  6 
 \end{bmatrix} + 3 \times \begin{bmatrix}
  7  \\
  8 \\
  9 
 \end{bmatrix} = \begin{bmatrix}
  30  \\
  36 \\
  42 
 \end{bmatrix}$$
 



## Test slide
<div style="float: left; width: 50%;">
Stuff here
</div>

<div style="float: right; width: 50%;">
More stuff here 
</div>