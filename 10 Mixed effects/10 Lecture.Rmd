---
title: "Mixed effects models"
subtitle: "Wheels within wheels"
author: "Samuel Robinson, Ph.D."
date: "December 10, 2020"
output: 
  beamer_presentation:
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
df_print: kable
header-includes: 
  - \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  - \useinnertheme{circles}
---

```{r setup, include=FALSE}
#Trick to get smaller R code size with out resorting to LaTeX text sizes
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message=TRUE, warning=TRUE, size = 'footnotesize')
library(tidyverse)
theme_set(theme_classic())
library(lme4)
# library(ggpubr)
# library(knitr)
# library(kableExtra)
# library(latex2exp)

set.seed(123)

#Generate data
n <- 150
ngroups <- 15
x <- runif(n,-10,10) #Single fixed effect predictor
g <- sample(letters[1:ngroups],n,TRUE) #Groups
intercept <- 1
slopeX <- 0.5
sigmaR <- 3 #Residual sigma 
sigmaG <- 5 #Group intercept sigma
sigmaG_slope <- abs(slopeX*2) #Slope sigma (half slope value)

#Correlated intercepts and slopes, using Choleski matrices
raneffs <- matrix(rnorm(ngroups*2,0,1),ncol=2) #Uncorrelated unit normals
slopeCor <- 0.7 #Intercept-slope correlation
corMat <- matrix(c(1,slopeCor,slopeCor,1),ncol=2) #Correlation matrix
cholCorMat <- chol(corMat) #Choleski transform of corMat
raneffs <- raneffs %*% cholCorMat #Induces correlation in slopes
raneffs <- raneffs * matrix(rep(c(sigmaG,sigmaG_slope),each=ngroups),ncol=2,
                            dimnames=list(letters[1:ngroups],c('Int','Slope'))) #Changes SD for each column
raneff_int <- model.matrix(~g-1) %*% raneffs[,1] #Intercept vector
raneff_slope <- model.matrix(~g-1) %*% raneffs[,2] #Slope vector

yhat <- intercept + slopeX*x + raneff_int + raneff_slope*x  #Expected value
y <- rnorm(n,yhat,sigmaR) #Data
dat <- data.frame(y,x,site=g) #Assemble into data frame

```

## Motivation

- What are mixed effects models?
  - Scary math (matrix algebra)
  - Variance partitioning
  - Fixed effects vs. Random effects
- Working with random effects
  - What should be a random effect?
  - Model validation
  - Hypothesis testing
  - Examples
- Exercise!

## What are mixed effects models?

Many different names:

1. Mixed effects models
2. Random effects models
3. Heirarchical models
4. Empirical/Bayesian heirarchical models
5. Latent variable models
6. Split-plot models\footnotemark

I will use the term _heirarchical models_, as this is the closest to what I will teach you

\footnotetext{Earlier form of variance partitioning}

## Scary math

Unfortunately, we need a review of matrix algebra in order to explain this:

- This is a matrix: $$ A = \begin{bmatrix}
1 & 4 & 7 \\
2 & 5 & 8 \\
3 & 6 & 9
\end{bmatrix}
$$
  
- This is a vector $$ b = \begin{bmatrix}
1 & 2 & 3 
\end{bmatrix}
$$
  
- Multiplying them looks like this:

$$ A \times b = Ab = 1 \times \begin{bmatrix}
1  \\
2 \\
3 
\end{bmatrix} + 2 \times \begin{bmatrix}
4  \\
5  \\
6 
\end{bmatrix} + 3 \times \begin{bmatrix}
7  \\
8 \\
9 
\end{bmatrix} = \begin{bmatrix}
30  \\
36 \\
42 
\end{bmatrix}$$

## Why do we call them "linear models"?

Involves a _linear mapping_ of coefficients onto a model matrix

Coefficients: $$ \beta = \begin{bmatrix}
0.1 & 1.8 & -0.03 
\end{bmatrix}
$$

Model matrix:

$$ X = \begin{bmatrix}
1 & 1 & 10 \\
1 & 1 & 12 \\
1 & 0 & 9 \\
\vdots & \vdots & \vdots
\end{bmatrix}
$$

Multiplying them looks like: 

$$ \hat{y} = X\beta = 
\begin{bmatrix}
1.60  \\
1.54 \\
-0.17 \\
\vdots
\end{bmatrix}$$

## This is exactly what R does to fit models:

```{r, echo=TRUE, size='tiny'}
head(dat)
m1 <- lm(y~x,data=dat) #Use variable x to predict y
summary(m1)
```

## This is exactly what R does to fit models (cont.):

```{r, echo=TRUE, size='tiny'}
head(model.matrix(m1))
coef(m1)
pred2 <- model.matrix(m1) %*% coef(m1) #predicted = matrix * coefs
head(data.frame(pred1=predict(m1),pred2)) #same thing!
```

## Groups are coded by "dummy variables" (0s and 1s)

```{r, echo=TRUE, size='tiny'}
head(dat)
m2 <- lm(y~site,data=dat) #Use variable site to predict y
head(model.matrix(m2)) #0s and 1s used to identify groups
coef(m2) #This uses the 1st site as the "control" group
```

## Structure of LMs... now with matrices!

- All linear models take the form:
\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{darkturquoise}{X}\textcolor{blue}{\beta} = \textcolor{blue}{b_0}\textcolor{darkturquoise}{1} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{x_1} ... + \textcolor{blue}{b_i}\textcolor{darkturquoise}{x_i} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

- $y$ is a vector of data you want to predict
- $\textcolor{orange}{\hat{y}}$ is a vector of _predicted values_ for $y$
- $\textcolor{darkturquoise}{X} = \textcolor{darkturquoise}{\{1,x_1...\}}$ is a matrix of _predictors_ for _y_
- $\textcolor{blue}{\beta} = \textcolor{blue}{\{b_0,b_1,...\}}$ is a vector of _coefficients_
- $y\sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})$ means:
  - "$y$ follows a Normal distribution with mean $\textcolor{orange}{\hat{y}}$ and SD $\textcolor{red}{\sigma}$"
  
## Fixed effects vs. Random effects

Say that $\textcolor{darkturquoise}{X}$ is a model matrix coding for 10 sites\footnotemark, and _y_ is something we're interested in predicting

::: columns

:::: column

\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{darkturquoise}{X}\textcolor{blue}{\beta} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

\hfill\newline

- Site coefficients ($\textcolor{blue}{\beta}$) are unrelated to each other
- $\textcolor{red}{\sigma}$ is the SD of _residuals_
- Site is a __fixed effect__

::::

:::: column

\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{darkturquoise}{X}\textcolor{purple}{\zeta} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma}) \\
\textcolor{purple}{\zeta} & \sim Normal(0,\textcolor{red}{\sigma_{site}})
\end{split}
\end{equation*}

- Site coefficients ($\textcolor{blue}{\beta}$) are related to each other via a _Normal_ distribution
- $\textcolor{red}{\sigma}$ is the SD of _residuals_, $\textcolor{red}{\sigma_{site}}$ is the SD of _sites_
- Site is a __random effect__

::::

:::


\footnotetext{Intercept is a separate variable}

## Mixed effects = fixed + random effects

A mixed effects model has both __fixed__ and __random__ effects

\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{darkturquoise}{X}\textcolor{blue}{\beta} + \textcolor{gray}{U}\textcolor{purple}{\zeta} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma}) \\
\textcolor{purple}{\zeta} & \sim Normal(0,\textcolor{red}{\sigma_{site}})
\end{split}
\end{equation*}

- $\textcolor{darkturquoise}{X}$ = fixed effects matrix (e.g. intercept, temperature)
- $\textcolor{blue}{\beta}$ = fixed effects coefficients (e.g. )
- $\textcolor{gray}{U}$ = random effects matrix (e.g. sites)
- $\textcolor{purple}{\zeta}$ = random effects coefficients
- $\textcolor{red}{\sigma}$, $\textcolor{red}{\sigma_{site}}$ = variance terms

## Mixed effect model example

Let's go back to our earlier example:

- We're interested in predicting _y_ using _x_ (fixed effects)
- Data was collected at a number of _sites_, which may affect _y_ "somehow"
- Effect of each site is normally distributed

```{r echo=TRUE, message=FALSE, warning=FALSE, size='tiny'}
head(dat)
library(lme4) #Mixed effects library 
mm1 <- lmer(y ~ x + (1|site),data=dat) #site is fit as "random intercepts"
```

## Mixed effect model example

::: columns

:::: column

```{r size='tiny'}
summary(mm1)
```

::::

:::: column

Results from `lmer` model:

- Random effects:
  - _residual_ and _site_ variance ($\textcolor{red}{\sigma}$, $\textcolor{red}{\sigma_{site}}$)
- Fixed effects: 
  - Intercept and slope estimates

::::

:::

## Mixed effect model example

Each site is 

```{r}
dat %>% mutate(pred=predict(mm1),gpred=predict(mm1,re.form=~0)) %>% 
  ggplot(aes(x=x,y=y,col=site))+geom_point()+geom_line(aes(y=pred))+
  geom_line(aes(y=gpred),col='black',size=2)
```